
# ---------------------------------------------------------------------------------------------
# MIT License
# Copyright (c) 2020, Solace Corporation, Ricardo Gomez-Ulmke (ricardo.gomez-ulmke@solace.com)
# ---------------------------------------------------------------------------------------------

---
  - debug:
      msg: "run_num = {{ run_num }}"

  - debug:
      msg:
        - "run="
        - "{{run}}"

  - name: "Final Run Spec: Infrastructure"
    set_fact:
      run_infrastrucure:
        cloud_provider: "{{ run.infrastructure.cloud_provider | default(test_spec.run_defaults.infrastructure.cloud_provider)}}"
        id: "{{ run.infrastructure.id | default(test_spec.run_defaults.infrastructure.id)}}"

  - name: "Final Run Spec: Load"
    set_fact:
      run_load:
        include: "{{ run.load.include | default(test_spec.run_defaults.load.include)}}"
        pub_num_topics: "{{ run.load.pub_num_topics | default(test_spec.run_defaults.load.pub_num_topics)}}"
        pub_msg_rate_per_second: "{{ run.load.pub_msg_rate_per_second | default(test_spec.run_defaults.load.pub_msg_rate_per_second)}}"
        pub_msg_payload_size_bytes: "{{ run.load.pub_msg_payload_size_bytes | default(test_spec.run_defaults.load.pub_msg_payload_size_bytes)}}"
        pubs_to_subs_ratio: "{{ run.load.pubs_to_subs_ratio | default(test_spec.run_defaults.load.pubs_to_subs_ratio)}}"
        pub_total_num_msgs: "calculate me..."

  - name: "Final Run Spec: Monitors"
    set_fact:
      run_monitors:
        latency:
          include: "{{ run.monitors.latency.include | default(test_spec.run_defaults.monitors.latency.include) }}"
          topic_prefix: "{{ run.monitors.latency.topic_prefix | default(test_spec.run_defaults.monitors.latency.topic_prefix) }}"
          warmup_secs: "{{ run.monitors.latency.warmup_secs | default(test_spec.run_defaults.monitors.latency.warmup_secs) }}"
          msg_rate_per_second: "{{ run.monitors.latency.msg_rate_per_second | default(test_spec.run_defaults.monitors.latency.msg_rate_per_second) }}"
          msg_payload_size_bytes: "{{ run.monitors.latency.msg_payload_size_bytes | default(test_spec.run_defaults.monitors.latency.msg_payload_size_bytes) }}"
          lg: "{{ run.monitors.latency.lg | default(test_spec.run_defaults.monitors.latency.lg) }}"
          lb: "{{ run.monitors.latency.lb | default(test_spec.run_defaults.monitors.latency.lb) }}"
        latency_brokernode:
          include: "{{ run.monitors.latency_brokernode.include | default(test_spec.run_defaults.monitors.latency_brokernode.include) }}"
          topic_prefix: "{{ run.monitors.latency_brokernode.topic_prefix | default(test_spec.run_defaults.monitors.latency_brokernode.topic_prefix) }}"
          warmup_secs: "{{ run.monitors.latency_brokernode.warmup_secs | default(test_spec.run_defaults.monitors.latency_brokernode.warmup_secs) }}"
          msg_rate_per_second: "{{ run.monitors.latency_brokernode.msg_rate_per_second | default(test_spec.run_defaults.monitors.latency_brokernode.msg_rate_per_second) }}"
          msg_payload_size_bytes: "{{ run.monitors.latency_brokernode.msg_payload_size_bytes | default(test_spec.run_defaults.monitors.latency_brokernode.msg_payload_size_bytes) }}"
          lg: "{{ run.monitors.latency_brokernode.lg | default(test_spec.run_defaults.monitors.latency_brokernode.lg) }}"
          lb: "{{ run.monitors.latency_brokernode.lb | default(test_spec.run_defaults.monitors.latency_brokernode.lb) }}"
        ping:
          include: "{{ run.monitors.ping.include | default(test_spec.run_defaults.monitors.ping.include) }}"
        vpn:
          include: "{{ run.monitors.vpn.include | default(test_spec.run_defaults.monitors.vpn.include) }}"

# write out a tmp/run_spec.json
# or, better: set env var RUN_SPEC, playbooks read that as input?
# the playbooks use that to go
# re-build _run.tests.sh
#
# re-build monitor/run.monitor.sh
#    - and call the scripts in background
# delete tmp/run_spec.json


  - name: "Run the Test: {{run.name}}"
    debug:
      msg:
        - "export UC_NON_PERSISTENT_INFRASTRUCTURE={{run_infrastrucure.cloud_provider}}.{{run_infrastrucure.id}}"
        - "export RUN_LOG_DIR={{RUN_LOG_DIR}}"
        - "export RUN_ID_PREFIX={{run.name}}-"
        - "run.tests.sh "

  - meta: end_play

  # - name: "Run 'sdkperf' ..."
  #   shell: >
  #     taskset -c 1,2
  #     {{sdkperf_nodes.sdkperf_root}}/sdkperf-c-x64/sdkperf_c.sh
  #     -cip={{broker_uri}}:{{sdkperf.broker_port}}
  #     -cn={{hostvars[inventory_hostname].boxname}}-1
  #     -cu={{ sdkperf.client_publisher_username }}@{{ sdkperf.broker_msg_vpn }}
  #     -ptl={{ monitor.latency.topic_prefix }}/{{STATS_NAME}}
  #     -stl={{ monitor.latency.topic_prefix }}/{{STATS_NAME}}
  #     -mn={{ sdkperf_msg_number }}
  #     -msa={{ monitor.latency.msg_payload_size_bytes }}
  #     -mr={{ monitor.latency.msg_rate_per_second }}
  #     -psm
  #     -l
  #     -lwu={{monitor.latency.warmup_secs}}
  #     -lb={{monitor.latency.lb}}
  #     -lg={{monitor.latency.lg}}
  #   register: sdkperf_result
  #   failed_when: sdkperf_result.stdout == ""


###
# The End.
